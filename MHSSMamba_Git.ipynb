{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2Wyq7c6a3yA",
    "outputId": "5a4e395f-229f-41aa-c37c-499b88dace6f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from operator import truediv\n",
    "import spectral\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import (IncrementalPCA, PCA)\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             cohen_kappa_score, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import legacy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def LoadHSIData(method):\n",
    "    data_path = os.path.join(os.getcwd(),'../HSI/')\n",
    "    if method == 'SA':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "        Num_Classes = 16\n",
    "        target_names = ['Weeds_1','Weeds_2','Fallow',\n",
    "                        'Fallow_rough_plow','Fallow_smooth', 'Stubble','Celery',\n",
    "                        'Grapes_untrained','Soil_vinyard_develop','Corn_Weeds',\n",
    "                        'Lettuce_4wk','Lettuce_5wk','Lettuce_6wk',\n",
    "                        'Lettuce_7wk', 'Vinyard_untrained','Vinyard_trellis']\n",
    "    elif method == 'PU':\n",
    "        HSI = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
    "        GT = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
    "        Num_Classes = 9\n",
    "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted','Soil','Bitumen',\n",
    "                        'Bricks','Shadows']\n",
    "    elif method == 'UH':\n",
    "      HSI = sio.loadmat(os.path.join(data_path, 'HU.mat'))['HSI']\n",
    "      GT = sio.loadmat(os.path.join(data_path, 'HU_gt.mat'))['gt']\n",
    "      Num_Classes = 15\n",
    "      target_names = ['Healthy grass', 'Stressed grass', 'Synthetic grass', 'Trees',\n",
    "                    'Soil', 'Water', 'Residential', 'Commercial', 'Road',\n",
    "                    'Highway', 'Railway', 'Parking Lot 1', 'Parking Lot 2',\n",
    "                    'Tennis Court', 'Running Track']\n",
    "    return HSI, GT, Num_Classes, target_names\n",
    "\n",
    "def DLMethod(method, HSI, NC = 75):\n",
    "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
    "    if method == 'PCA': ## PCA\n",
    "        pca = PCA(n_components = NC, whiten = True)\n",
    "        RHSI = pca.fit_transform(RHSI)\n",
    "        RHSI = np.reshape(RHSI, (HSI.shape[0], HSI.shape[1], NC))\n",
    "    elif method == 'iPCA': ## Incremental PCA\n",
    "        n_batches = 256\n",
    "        inc_pca = IncrementalPCA(n_components = NC)\n",
    "        for X_batch in np.array_split(RHSI, n_batches):\n",
    "          inc_pca.partial_fit(X_batch)\n",
    "        X_ipca = inc_pca.transform(RHSI)\n",
    "        RHSI = np.reshape(X_ipca, (HSI.shape[0], HSI.shape[1], NC))\n",
    "    return RHSI\n",
    "\n",
    "def TrTeSplit(HSI, GT, trRatio, vrRatio, teRatio, randomState=345):\n",
    "    Tr, Te, TrC, TeC = train_test_split(HSI, GT, test_size=teRatio,\n",
    "                                        random_state=randomState, stratify=GT)\n",
    "    totalTrRatio = trRatio + vrRatio\n",
    "    new_vrRatio = vrRatio / totalTrRatio\n",
    "    Tr, Va, TrC, VaC = train_test_split(Tr, TrC, test_size=new_vrRatio,\n",
    "                                        random_state=randomState, stratify=TrC)\n",
    "    return Tr, Va, Te, TrC, VaC, TeC\n",
    "HSID = \"UH\"\n",
    "DLM = \"PCA\"\n",
    "WS = 4\n",
    "teRatio = 0.20\n",
    "vrRatio = 0.50\n",
    "trRatio = 0.50\n",
    "k = 15\n",
    "adam = tf.keras.optimizers.legacy.Adam(lr = 0.001, decay = 1e-06)\n",
    "epochs = 50\n",
    "batch_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JS3nGJaRhPtR"
   },
   "outputs": [],
   "source": [
    "def ImageCubes(HSI, GT, WS=WS, removeZeroLabels=True):\n",
    "    num_rows, num_cols, num_bands = HSI.shape\n",
    "    margin = int(WS / 2)\n",
    "    padded_data = np.pad(HSI, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n",
    "    image_cubes = np.zeros((num_rows * num_cols, WS, WS, num_bands))\n",
    "    patchesLabels = np.zeros((num_rows * num_cols))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, num_rows + margin):\n",
    "        for c in range(margin, num_cols + margin):\n",
    "            cube = padded_data[r - margin: r + margin, c - margin: c + margin, :]\n",
    "            image_cubes[patchIndex, :, :, :] = cube\n",
    "            patchesLabels[patchIndex] = GT[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "      image_cubes = image_cubes[patchesLabels>0,:,:,:]\n",
    "      patchesLabels = patchesLabels[patchesLabels>0]\n",
    "      patchesLabels -= 1\n",
    "    return image_cubes, patchesLabels\n",
    "def ClassificationReports(TeC, Te_Pred, target_names):\n",
    "    classification = classification_report(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1), target_names = target_names)\n",
    "    oa = accuracy_score(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    confusion = confusion_matrix(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    list_diag = np.diag(confusion)\n",
    "    list_raw_sum = np.sum(confusion, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    aa = np.mean(each_acc)\n",
    "    kappa = cohen_kappa_score(np.argmax(TeC, axis=1), np.argmax(Te_Pred, axis=1))\n",
    "    return classification, confusion, oa*100, each_acc*100, aa*100, kappa*100\n",
    "def CSVResults(file_name, classification, confusion, Tr_Time, Te_Time, DL_Time, kappa, oa, aa, each_acc):\n",
    "    classification = str(classification)\n",
    "    confusion = str(confusion)\n",
    "    with open(file_name, 'w') as CSV_file:\n",
    "      CSV_file.write('{} Tr_Time'.format(Tr_Time))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Te_Time'.format(Te_Time))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} DL_Time'.format(DL_Time))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Overall accuracy (%)'.format(oa))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{} Average accuracy (%)'.format(aa))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(classification))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(each_acc))\n",
    "      CSV_file.write('\\n')\n",
    "      CSV_file.write('{}'.format(confusion))\n",
    "    return CSV_file\n",
    "HSI, GT, Num_Classes, target_names = LoadHSIData(HSID)\n",
    "start = time.time()\n",
    "RDHSI = DLMethod(DLM, HSI, NC = k)\n",
    "end = time.time()\n",
    "DL_Time = end - start\n",
    "CRDHSI, CGT = ImageCubes(RDHSI, GT, WS = WS)\n",
    "Tr, Va, Te, TrC, VaC, TeC = TrTeSplit(CRDHSI, CGT, trRatio, vrRatio, teRatio)\n",
    "TrC = to_categorical(TrC)\n",
    "VaC = to_categorical(VaC)\n",
    "TeC = to_categorical(TeC)\n",
    "\n",
    "class SpectralSpatialTokenGeneration(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels, **kwargs):\n",
    "        super(SpectralSpatialTokenGeneration, self).__init__(**kwargs)\n",
    "        self.spatial_tokens = Dense(out_channels)\n",
    "        self.spectral_tokens = Dense(out_channels)\n",
    "    def call(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        spatial_tokens = self.spatial_tokens(tf.reshape(tf.transpose(x, [0, 2, 3, 1]), [tf.shape(x)[0], H * W, C]))\n",
    "        spectral_tokens = self.spectral_tokens(tf.reshape(tf.transpose(x, [0, 1, 2, 3]), [tf.shape(x)[0], H * W, C]))\n",
    "        return spatial_tokens, spectral_tokens\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.all_head_size = self.num_heads * self.head_dim\n",
    "        self.query = Dense(self.all_head_size)\n",
    "        self.key = Dense(self.all_head_size)\n",
    "        self.value = Dense(self.all_head_size)\n",
    "        self.dropout = Dropout(dropout)\n",
    "    def call(self, query, key, value):\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        query_proj = self.query(query)\n",
    "        key_proj = self.key(key)\n",
    "        value_proj = self.value(value)\n",
    "        query_proj = tf.reshape(query_proj, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        query_proj = tf.transpose(query_proj, [0, 2, 1, 3])\n",
    "        key_proj = tf.reshape(key_proj, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        key_proj = tf.transpose(key_proj, [0, 2, 1, 3])\n",
    "        value_proj = tf.reshape(value_proj, [batch_size, -1, self.num_heads, self.head_dim])\n",
    "        value_proj = tf.transpose(value_proj, [0, 2, 1, 3])\n",
    "        attention_scores = tf.matmul(query_proj, key_proj, transpose_b=True) / tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, value_proj)\n",
    "        attention_output = tf.transpose(attention_output, [0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, [batch_size, -1, self.all_head_size])\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        return attention_output\n",
    "\n",
    "class SpectralSpatialFeatureEnhancement(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels, **kwargs):\n",
    "        super(SpectralSpatialFeatureEnhancement, self).__init__(**kwargs)\n",
    "        self.spatial_gate = Sequential([\n",
    "            Dense(out_channels),\n",
    "            Activation('sigmoid'),\n",
    "            Reshape((1, out_channels))\n",
    "        ])\n",
    "        self.spectral_gate = Sequential([\n",
    "            Dense(out_channels),\n",
    "            Activation('sigmoid'),\n",
    "            Reshape((1, out_channels))\n",
    "        ])\n",
    "    def call(self, spatial_tokens, spectral_tokens, center_tokens):\n",
    "        spatial_enhanced = spatial_tokens * self.spatial_gate(center_tokens)\n",
    "        spectral_enhanced = spectral_tokens * self.spectral_gate(center_tokens)\n",
    "        return spatial_enhanced, spectral_enhanced\n",
    "\n",
    "class StateSpaceModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_dim, **kwargs):\n",
    "        super(StateSpaceModel, self).__init__(**kwargs)\n",
    "        self.state_dim = state_dim\n",
    "        self.state_transition = Dense(state_dim)\n",
    "        self.state_update = Dense(state_dim)\n",
    "    def call(self, x):\n",
    "        state = tf.zeros([tf.shape(x)[0], self.state_dim])\n",
    "        for t in range(tf.shape(x)[1]):\n",
    "            state = self.state_transition(state) + self.state_update(x[:, t, :])\n",
    "        return state\n",
    "\n",
    "class SSMambaModel(tf.keras.Model):\n",
    "    def __init__(self, out_channels, num_heads, state_dim, dropout=0.1, **kwargs):\n",
    "        super(SSMambaModel, self).__init__(**kwargs)\n",
    "        self.token_generation = SpectralSpatialTokenGeneration(out_channels)\n",
    "        self.multi_head_attention = MultiHeadAttention(out_channels, num_heads, dropout)\n",
    "        self.feature_enhancement = SpectralSpatialFeatureEnhancement(out_channels)\n",
    "        self.state_space_model = StateSpaceModel(state_dim)\n",
    "        self.dense = Dense(units=128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.dropout = Dropout(0.4)\n",
    "        self.classifier = Dense(Num_Classes, activation='softmax')\n",
    "    def call(self, x):\n",
    "        spatial_tokens, spectral_tokens = self.token_generation(x)\n",
    "        center_tokens = spatial_tokens[:, x.shape[1] // 2, :]\n",
    "        spatial_enhanced, spectral_enhanced = self.feature_enhancement(spatial_tokens, spectral_tokens, center_tokens)\n",
    "        attention_output = self.multi_head_attention(spatial_enhanced, spectral_enhanced, spectral_enhanced)\n",
    "        state_output = self.state_space_model(attention_output)\n",
    "        output = self.classifier(state_output)\n",
    "        return output\n",
    "\n",
    "model = SSMambaModel(out_channels=64, num_heads=4, state_dim=128, dropout=0.1)\n",
    "_ = model(Tr[:batch_size])\n",
    "total_params = model.count_params()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "start = time.time()\n",
    "history = model.fit(x=Tr, y=TrC, batch_size=batch_size, epochs=epochs, validation_data=(Va, VaC))\n",
    "end = time.time()\n",
    "Tr_Time = end - start\n",
    "start = time.time()\n",
    "Te_Pre = model.predict(Te)\n",
    "end = time.time()\n",
    "Te_Time = end - start\n",
    "classification,Confusion,OA,Per_Class,AA,Kappa = ClassificationReports(TeC, Te_Pre, target_names)\n",
    "file_name = f\"{HSID}_{teRatio}_{k}_{WS}_{DLM}_Classification_Report.csv\"\n",
    "CSV_file = CSVResults(file_name, classification, Confusion, Tr_Time, Te_Time, DL_Time, Kappa, OA, AA, Per_Class)\n",
    "# outputs = GT_Plot(CRDHSI, GT, model, WS, k)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(outputs, cmap='nipy_spectral')\n",
    "# plt.axis('off')\n",
    "# file_name = f\"{HSID}_{teRatio}_{k}_{WS}_{DLM}_Ground_Truths.png\"\n",
    "# plt.savefig(file_name, dpi=500, format='png', bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "hsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
